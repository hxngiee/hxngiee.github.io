---
layout: post
title: Instance-aware Image-to-Image Translation - 작성중  
subtitle: Utilizing the set structured side information  
thumbnail-img: /assets/img/instagan_overview.jpg 
tags: [Paper Review]
use_math: true
comments: true
---

## Table of contents
- [Abstract](#abstract)  
- [Datasets](#datasets)  
- [Architecture](#architecture)  
- [Generator](#generator)  
- [Discriminator](#discriminator)  
- [Training Loss](#training-loss)  


## Abstract  
- 기존 I2I model 들은 multiple target instances 를 변환하거나 Geometry Change를 크게하는 task 에 대해 어려움을 겪고 있음  
- Instagan은 이미지와 instance 정보(object segmentation mask)를 활용하여 multiple target instances, Geometry Change 에 대한 translate 을 시도
- Segmentation masks를 이용해 instance를 translate 하기 위한 필요 조건과 네트워크 구조 제시
- 네트워크가 instances와 background를 구분할 수 있도록 돕는 Context preserving loss 제안
- Sequential mini batch training기법을 통해 Multiple instances에 대한 일반화 성능을 높이고 제한된 GPU로 효율적으로 학습할 수 있는 방법론 제안


## Datasets  
<center>
<img src="/assets/img/instagan-datasets.jpg" alt="Component model visualisation">
</center>  
- 객체 탐지 (object detection), 세그먼테이션 (segmentation), 키포인트 탐지 (keypoint detection) 등을 위한 MS COCO Dataset 사용
- 이미지 한장마다 여러 장의 instances segmentation mask가 함께 존재
- Edge line 이 선명하고 pixel intensity가 확실한 것이 특징


## Architecture  
<center>
<img src="/assets/img/instagan-architecture.png" alt="Component model visualisation">
</center>  


### CycleGAN과 다른 점  
- Instagan은 additional한 instance 정보를 활용한다는 점에서 기존 I2I 와 차이가 있음  
    - XxA, YxB간의 attribute augmented space간 joint mapping 을 학습하는 방식  
- 본 방식은 G가 서로 다른 domain의 instance 정보를 더 잘 disentangle 하고 정확하고 섬세하게 translation 을 하도록 도움  
- cycleGAN과 다르게 single mapping 만으로도 학습이 가능함  
    - 기존의 경우 cycle loss(Gyx(Gxy(x)) ~ x)를 통해 2개의 mapping function(Gxy, Gyx)를 jointly 하게 학습


## Generator  
<center>
<img src="/assets/img/instagan-generator.jpg" alt="Component model visualisation">
</center>  
- x 와 a 를 encode 하여 y’ 과 b’으로 변환  
- Multi instance mask 이미지를 Image translation 에 활용하기 위한 조건  
    - y’은 a에 있는 각각의 instance에 대해 `Permutation-invariant` 성질을 띄어야 함
        - Input 벡터 요소의 순서와 상관없이 같은 출력을 생성하는 모델
    - b’은 a에 있는 각각의 instance에 대해 `permutation-equivariant` 성징을 띄어야 함
        - input 이 바뀌면 output 이 동일한 방법으로 바뀌는 것
- Feature encoding
    - Image feature extractor fgx와 attributes feature extractor fga 각각 사용
    - 여러 개의 instance mask 이미지는 Attribute feature extracto 에 의해 feature가 추출되고 이를 aggregation 하여 permutation invariant 한 feature set 구성
    - Image와 attribute feature를 concat하여 G에 전달


## Discriminator  
<center>
<img src="/assets/img/instagan-discriminator.jpg" alt="Component model visualisation">
</center>  
- x 와 a 를 encode 하여 해당 정보가 원본 Domain 의 것인지 아닌지 확인
- Mask 이미지들은 D 에 대해 permutation invariant 성질을 충족
- 네트워크내에서 x 와 a 를 처리하는 component 를 나눈 뒤 두 정보를 joint 하게 학습
    - x 와 a 의 관계성에 대해 학습하는 역할
    - G 와 D 에서 x 와 a 를 joint 하게 학습시킴으로써 G 가 target segmentation mask 에 맞는 image instance 를 잘 생성할 수 있었음


## Training Loss  
- I2I Loss 설계시 주의사항
    - 변화시켜야 하는 것 : instance 의 shape 와 style 을 target domain 의 특성으로 변화
    - 유지되어야 하는 것 : 원본의 background 혹은 instance’s domain independent characteristics

- Domain loss 로 일반 GAN Loss 사용
    - 생성된 이미지가 target domain 의 style 을 따르도록 유도
- Content loss로 Cycle consistency loss, Identity mapping loss 사용
    - 이미지 변환시 생성된 이미지가 원본의 content 를 잃지 않도록 유도
- Context preserving loss
    - **수식**
    - Background 를 유지한 채, instance만 변화되도록 유도
    - Image translation 시 instance 의 segmentation 영역이 달라져 , 두 이미지에서의 background 정의가 달라지는 문제 발생
        - 공통으로 background 라 정의된 부분만 backgroun 라 정의
    - real_A_seg와 fake_B_seg 를 더해 새로운 segmentation map 을 만든 후 값을 뒤집어줌
        - background : 1, instance: 0


## Sequential Mini-Batch Translation

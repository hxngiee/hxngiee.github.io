---
layout: post
title: Learning Transferable Visual Models From Natural Language Supervision - 작성중
subtitle: Impressive zero shot performance for distribution shift and domain generalization
# thumbnail-img: /assets/img/pr_pono.png 
tags: [Paper Review]
use_math: true
comments: true
---

## Table of contents
- [Abstract](#abstract)

## Abstract
- introduce a concept of using zero shot in computer vision
- take nlp paradigm to computer vision 

## Method
### (1) Contrastive pre-training

<center>
<img src="/assets/img/clip-main-diagrams.jpg" alt="Component model visualisation">
</center>  

- Associated with they image/text pairs
- Image Part
  - data augmentation 
  - Image Encoder : ResNet and vision transformer
    - get certain representation at the output 
    - Linear Projection and finally inside contrastive embedding space
- Text Part
  - Text Encoder : Vaswani transformer
    - embed text sequence here 
    - Layer Normalization and use linear projection layer into the embedding space  

Linear Projectoin Layer를 통해 embedding space를 contrastive embedding space로 가져옴  


${<} I_i,T_i {>}$  
\begin{cases}
1,  & \text{if i==j} \\
0, & \text{otherwise}
\end{cases}

over the row  

$$l_i^{v \rightarrow u} = - log\frac{\exp{<} v_i,u_i {>}/\tau)}{\sum_{k=1}^{N}\exp{<} v_i,u_i {>}/\tau}$$ 

\tau : temperature coefficient, just modifies the softmax distribution to make it a bit more steep
familiar softmax : sum of all of I_1,T_i and get the probability distribution 
-log : cross entropy loss over that softmax distribution

over the column  
$$l_i^{v \rightarrow u} = - log\frac{\exp{<} u_i,v_i {>}/\tau)}{\sum_{k=1}^{N}\exp{<} u_i,v_i {>}/\tau}$$ 


Final Loss Vector
weight combination of those two and sum over the whole batch and avg 
$$L = \frac{1}{N}{\sum_{i=1}^{N}(\lambda l_i^{v\rightarrow u}+(1-\lambda)l_i^{u\rightarrow v}}$$

### (2) Create dataset classifier from label text


Q. 이 논문을 어떻게 이용할 수 있을까  
Q. 참고하고 싶은 다른 레퍼런스
